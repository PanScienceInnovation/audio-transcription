#!/usr/bin/env python3
"""
Script to delete duplicate files from MongoDB and S3, keeping only one per duplicate group.

Reads duplicate findings from a JSON file (generated by check_duplicate_files.py) and
deletes all duplicates except one, which is kept based on a selection strategy.

Environment Variables Required:
- MONGODB_URI: MongoDB connection URI (default: mongodb://localhost:27017/)
- MONGODB_DATABASE: MongoDB database name (default: transcription_db)
- MONGODB_COLLECTION: MongoDB collection name (default: transcriptions)
- S3_BUCKET_NAME: S3 bucket name (default: transcription-audio-files)
- S3_REGION: S3 region (default: us-east-1)
- AWS_ACCESS_KEY_ID or ACCESS_KEY_ID: AWS access key
- AWS_SECRET_ACCESS_KEY or SECRET_ACCESS_KEY: AWS secret key
"""
import os
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

# Load environment variables from .env file if it exists
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# Add project root to path to import utils
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.storage import StorageManager


def parse_created_at(created_at_str: str) -> datetime:
    """Parse created_at string to datetime object."""
    try:
        # Try ISO format first
        if 'T' in created_at_str:
            return datetime.fromisoformat(created_at_str.replace('Z', '+00:00'))
        # Try other formats if needed
        return datetime.strptime(created_at_str, '%Y-%m-%d %H:%M:%S')
    except Exception as e:
        # Return a very old date as fallback
        return datetime(1970, 1, 1)


def select_document_to_keep(documents: List[Dict[str, Any]], strategy: str = 'oldest') -> Dict[str, Any]:
    """
    Select which document to keep from a list of duplicates.
    
    Strategies:
    - 'oldest': Keep the document with the earliest created_at (default)
    - 'newest': Keep the document with the latest created_at
    - 'not_flagged': Keep the document that is not flagged (if any), otherwise oldest
    - 'has_assignment': Keep the document with assigned_user_id (if any), otherwise oldest
    
    Args:
        documents: List of duplicate documents
        strategy: Selection strategy
        
    Returns:
        The document to keep
    """
    if len(documents) == 0:
        return None
    
    if len(documents) == 1:
        return documents[0]
    
    if strategy == 'oldest':
        # Keep the oldest document (earliest created_at)
        return min(documents, key=lambda d: parse_created_at(d.get('created_at', '1970-01-01')))
    
    elif strategy == 'newest':
        # Keep the newest document (latest created_at)
        return max(documents, key=lambda d: parse_created_at(d.get('created_at', '1970-01-01')))
    
    elif strategy == 'not_flagged':
        # Prefer documents that are not flagged
        not_flagged = [d for d in documents if not d.get('is_flagged', False)]
        if not_flagged:
            # If multiple not flagged, keep the oldest among them
            return min(not_flagged, key=lambda d: parse_created_at(d.get('created_at', '1970-01-01')))
        else:
            # All are flagged, keep the oldest
            return min(documents, key=lambda d: parse_created_at(d.get('created_at', '1970-01-01')))
    
    elif strategy == 'has_assignment':
        # Prefer documents with assigned_user_id
        has_assignment = [d for d in documents if d.get('assigned_user_id')]
        if has_assignment:
            # If multiple have assignment, keep the oldest among them
            return min(has_assignment, key=lambda d: parse_created_at(d.get('created_at', '1970-01-01')))
        else:
            # None have assignment, keep the oldest
            return min(documents, key=lambda d: parse_created_at(d.get('created_at', '1970-01-01')))
    
    else:
        # Default to oldest
        return min(documents, key=lambda d: parse_created_at(d.get('created_at', '1970-01-01')))


def load_duplicates_json(json_path: str) -> Dict[str, Any]:
    """Load duplicates from JSON file."""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except FileNotFoundError:
        print(f"‚ùå Error: File not found: {json_path}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"‚ùå Error: Invalid JSON in {json_path}: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Error reading JSON file: {e}")
        sys.exit(1)


def delete_duplicates(json_path: str, strategy: str = 'oldest', dry_run: bool = False):
    """
    Delete duplicate files, keeping one per group.
    
    Args:
        json_path: Path to JSON file with duplicate findings
        strategy: Strategy for selecting which document to keep
        dry_run: If True, only show what would be deleted without actually deleting
    """
    print("="*80)
    print("üóëÔ∏è  Delete Duplicate Files")
    print("="*80)
    print()
    
    if dry_run:
        print("‚ö†Ô∏è  DRY RUN MODE - No files will be deleted")
        print()
    
    # Load duplicates
    print(f"üìÇ Loading duplicates from: {json_path}")
    duplicates_data = load_duplicates_json(json_path)
    
    duplicates = duplicates_data.get('duplicates', {})
    total_groups = len(duplicates)
    
    if total_groups == 0:
        print("‚úÖ No duplicates found in the JSON file.")
        return
    
    print(f"üìä Found {total_groups} duplicate group(s)")
    print(f"   Strategy: {strategy}")
    print()
    
    # Initialize storage manager
    print("üì¶ Initializing StorageManager...")
    storage_manager = StorageManager()
    
    if not storage_manager.collection:
        print("‚ùå Error: MongoDB not initialized. Please check MongoDB connection.")
        sys.exit(1)
    
    if not storage_manager.s3_client and not dry_run:
        print("‚ö†Ô∏è  Warning: S3 client not initialized. S3 deletions will be skipped.")
        print("   Please check AWS credentials if you want to delete from S3.")
    
    print("‚úÖ StorageManager initialized successfully")
    print()
    
    # Process each duplicate group
    total_deleted = 0
    total_kept = 0
    total_errors = 0
    errors = []
    
    for filename, documents in duplicates.items():
        if len(documents) <= 1:
            continue  # Skip if not actually duplicates
        
        print(f"üìÅ Processing: {filename} ({len(documents)} duplicates)")
        
        # Select document to keep
        keep_doc = select_document_to_keep(documents, strategy)
        keep_id = keep_doc['document_id']
        
        print(f"   ‚úÖ Keeping: {keep_id} (created: {keep_doc.get('created_at', 'N/A')})")
        total_kept += 1
        
        # Delete the rest
        to_delete = [d for d in documents if d['document_id'] != keep_id]
        
        for doc in to_delete:
            doc_id = doc['document_id']
            print(f"   üóëÔ∏è  Deleting: {doc_id} (created: {doc.get('created_at', 'N/A')})")
            
            if not dry_run:
                result = storage_manager.delete_transcription(doc_id)
                
                if result.get('success'):
                    total_deleted += 1
                    s3_deleted = result.get('s3_deleted', False)
                    if s3_deleted:
                        print(f"      ‚úÖ Deleted from MongoDB and S3")
                    else:
                        print(f"      ‚úÖ Deleted from MongoDB (S3 may have been skipped)")
                else:
                    total_errors += 1
                    error_msg = result.get('error', 'Unknown error')
                    errors.append(f"{filename} ({doc_id}): {error_msg}")
                    print(f"      ‚ùå Error: {error_msg}")
            else:
                total_deleted += 1
                print(f"      [DRY RUN] Would delete from MongoDB and S3")
        
        print()
    
    # Print summary
    print("="*80)
    print("üìä DELETION SUMMARY")
    print("="*80)
    print(f"   Total duplicate groups: {total_groups}")
    print(f"   ‚úÖ Documents kept: {total_kept}")
    print(f"   üóëÔ∏è  Documents deleted: {total_deleted}")
    print(f"   ‚ùå Errors: {total_errors}")
    print("="*80)
    
    if errors:
        print("\n‚ùå Files with errors:")
        for error in errors[:20]:  # Show first 20 errors
            print(f"   - {error}")
        if len(errors) > 20:
            print(f"   ... and {len(errors) - 20} more errors")
    
    if dry_run:
        print("\n‚ö†Ô∏è  This was a DRY RUN - no files were actually deleted")
        print("   Run without --dry-run to perform the deletions")
    else:
        print("\n‚ú® Deletion completed!")
    
    return total_deleted, total_kept, total_errors


def main():
    """Main function."""
    parser = argparse.ArgumentParser(
        description='Delete duplicate files from MongoDB and S3',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Selection Strategies:
  oldest        Keep the document with the earliest created_at (default)
  newest        Keep the document with the latest created_at
  not_flagged   Keep the document that is not flagged (if any), otherwise oldest
  has_assignment Keep the document with assigned_user_id (if any), otherwise oldest

Examples:
  # Dry run to see what would be deleted
  python delete_duplicate_files.py duplicates.json --dry-run
  
  # Delete duplicates, keeping the oldest
  python delete_duplicate_files.py duplicates.json
  
  # Delete duplicates, keeping the newest
  python delete_duplicate_files.py duplicates.json --strategy newest
  
  # Delete duplicates, preferring non-flagged documents
  python delete_duplicate_files.py duplicates.json --strategy not_flagged
        """
    )
    
    parser.add_argument(
        'json_file',
        type=str,
        nargs='?',  # Make it optional
        default='duplicates_by_filename_20251210_162553.json',  # Default JSON file path
        help='Path to JSON file with duplicate findings (from check_duplicate_files.py)'
    )
    
    parser.add_argument(
        '--strategy',
        type=str,
        choices=['oldest', 'newest', 'not_flagged', 'has_assignment'],
        default='oldest',
        help='Strategy for selecting which document to keep (default: oldest)'
    )
    
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Show what would be deleted without actually deleting'
    )
    
    args = parser.parse_args()
    
    # Check if file exists
    json_path = Path(args.json_file)
    # If relative path, make it relative to script directory
    if not json_path.is_absolute():
        json_path = Path(__file__).parent / json_path
    if not json_path.exists():
        print(f"‚ùå Error: File not found: {json_path}")
        sys.exit(1)
    
    # Ask for confirmation if not dry run
    if not args.dry_run:
        print("‚ö†Ô∏è  WARNING: This will permanently delete files from MongoDB and S3!")
        print("   Make sure you have backed up the data if needed.")
        print()
        response = input("Do you want to continue? (yes/no): ").strip().lower()
        
        if response not in ['yes', 'y']:
            print("‚ùå Deletion cancelled by user.")
            sys.exit(0)
        print()
    
    try:
        delete_duplicates(str(json_path), args.strategy, args.dry_run)
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Deletion interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Fatal error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

